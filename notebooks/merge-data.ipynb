{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read in cleaned data sets\n",
    "\n",
    "\n",
    "df1 = pd.read_csv('../data/csv/permitted_wells_cleaned.csv', converters={'id': lambda x: str(x.strip()),\n",
    "                                                                'zip': lambda x: str(x.strip()),\n",
    "                                                                'add_zip': lambda x: str(x.strip()),\n",
    "                                                                'city': lambda x: str(x.strip()),\n",
    "                                                                'add': lambda x: str(x.strip())})\n",
    "\n",
    "\n",
    "\n",
    "df2 = pd.read_csv('../data/csv/sampled_wells_cleaned.csv', converters={'id': lambda x: str(x.strip()),\n",
    "                                                                'zip': lambda x: str(x.strip()),\n",
    "                                                                'city': lambda x: str(x.strip()),\n",
    "                                                                'add': lambda x: str(x.strip())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joins df2 to df1 on the following columns\n",
    "\n",
    "all_wells = pd.merge(df1, df2, on='id', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id             8145\n",
       "add_zip        8145\n",
       "city_x         8145\n",
       "zip_x          8145\n",
       "X              8145\n",
       "Y              8145\n",
       "depth          7834\n",
       "perm_date      8034\n",
       "add_x          8145\n",
       "year_built     8034\n",
       "add_y           726\n",
       "city_y          726\n",
       "state           726\n",
       "zip_y           726\n",
       "altid           726\n",
       "date            726\n",
       "ar              726\n",
       "ph              726\n",
       "date_tested     726\n",
       "year_tested     726\n",
       "group           726\n",
       "group_five      726\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks the number of valid entries in each column\n",
    "\n",
    "all_wells.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wells.rename(columns={'date': 'date_tested', 'city_x': 'city', 'zip_x': 'zip', 'add_x': 'add', 'year': 'year_tested'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates single address string for geocoding\n",
    "\n",
    "all_wells['full_add'] = all_wells['add'] + ', ' + all_wells['city'] + ', ' + 'NC ' + all_wells['zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset arsenic data for initial modeling process\n",
    "\n",
    "ar = all_wells[all_wells['group_five'] < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X    0\n",
       "Y    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checks the number of missing values in the new columns\n",
    "\n",
    "ar[['X', 'Y']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = ar[['id','full_add', 'year_built','date_tested', 'year_tested', 'X', 'Y', 'ar', 'group', 'group_five', 'depth', 'ph']]\n",
    "\n",
    "all_wells = all_wells[['id','full_add', 'year_built','date_tested', 'year_tested', 'X', 'Y', 'ar', 'group', 'group_five', 'depth', 'ph']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data for samples and newly built wells after 2017 will have to be geocoded, return once full data is obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#api_key = \"AIzaSyD4MWa0YgnE8mvIIxxTqJzMbzqippwbOFs\"\n",
    "#gmaps_key = googlemaps.Client(key=api_key)\n",
    "\n",
    "# geocodes useing full address ('full_add') for the well dataframe, \n",
    "# outputs X and Y coordinates into seperate new geoX and geoY columns\n",
    "# the geocode function will use the googlemaps library and geocode api to geocode the addresses\n",
    "\n",
    "#def geocode(row):\n",
    "#    try:\n",
    "#        result = gmaps_key.geocode(row['full_add'])\n",
    "#        geoX = result[0]['geometry']['location']['lng']\n",
    "#        geoY = result[0]['geometry']['location']['lat']\n",
    "#        return pd.Series([geoX, geoY])\n",
    "#    except:\n",
    "#        return pd.Series([np.nan, np.nan])\n",
    "\n",
    "# applies the geocode function to the well dataframe\n",
    "# the geocode function will create two new columns, geoX and geoY, in the well dataframe\n",
    "\n",
    "#well[['geoX', 'geoY']] = well.apply(geocode, axis=1)\n",
    "\n",
    "# check the number of missing values in the new columns\n",
    "\n",
    "#well[['geoX', 'geoY']].isnull().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for records missing values for X and Y, inserts the newly geocoded X and Y values into the original X and Y columns\n",
    "# thus retaining the already geocoded X and Y values for records that already have them\n",
    "\n",
    "#well['X'] = np.where(well['X'].isnull(), well['geoX'], well['X'])\n",
    "#well['Y'] = np.where(well['Y'].isnull(), well['geoY'], well['Y'])\n",
    "\n",
    "# check the number of missing values in the new columns\n",
    "\n",
    "#well[['X', 'Y']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X    0\n",
       "Y    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop records with missing values for X and Y\n",
    "\n",
    "#well = well.dropna(subset=['X', 'Y'])\n",
    "\n",
    "# check the number of missing values in the new columns\n",
    "\n",
    "#well[['X', 'Y']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_wells.to_csv(\"../data/gis/point/all_wells.csv\", index=False)\n",
    "\n",
    "ar.to_csv(\"../data/gis/point/ar_samples.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "58a2e68265e5871579dff07da96778d0132892d6744ab693af39e5915174ef8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
